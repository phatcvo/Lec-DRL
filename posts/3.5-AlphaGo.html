<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>alphago</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="3.5-AlphaGo_files/libs/clipboard/clipboard.min.js"></script>
<script src="3.5-AlphaGo_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="3.5-AlphaGo_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="3.5-AlphaGo_files/libs/quarto-html/popper.min.js"></script>
<script src="3.5-AlphaGo_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="3.5-AlphaGo_files/libs/quarto-html/anchor.min.js"></script>
<link href="3.5-AlphaGo_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="3.5-AlphaGo_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="3.5-AlphaGo_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="3.5-AlphaGo_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="3.5-AlphaGo_files/libs/bootstrap/bootstrap-bb8a42f168430693d1c0fc26666c4cdf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="alphago" class="level1">
<h1>AlphaGo</h1>
<p><strong>Go</strong> is an ancient two-opponents board game, where each player successively places stones on a 19x19 grid. When a stone is surrounded by four opponents, it dies. The goal is to ensure strategical position in order to cover the biggest territory. There are around <span class="math inline">\(10^{170}\)</span> possible states and 250 actions available at each turn (<span class="math inline">\(10^{761}\)</span> possible games), making it a much harder game than chess for a computer (35 possible actions, <span class="math inline">\(10^{120}\)</span> possible games). A game lasts 150 moves on average (80 in chess). Up until 2015 and <strong>AlphaGo</strong>, Go AIs could not compete with world-class experts, and people usually considered AI would need at least another 20 years to solve it. They were wrong.</p>
<div id="fig-go" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-go-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/go.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-go-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Example of a Go board.
</figcaption>
</figure>
</div>
<section id="minimax-and-alpha-beta" class="level2">
<h2 class="anchored" data-anchor-id="minimax-and-alpha-beta">Minimax and Alpha-Beta</h2>
<div id="fig-minimax" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-minimax-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/minimax.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-minimax-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Zero-sum game.
</figcaption>
</figure>
</div>
<p>The <strong>Minimax</strong> algorithm is a classical algorithm to find the optimal strategy in <strong>zero sum games</strong>, where what one player wins is lost by the other (basically all games except collaborative games). Minimax expands the whole game tree, simulating the aternating moves of the MAX (you) and MIN (your opponent) players. The final outcome (win = +1 or lose = -1, but it could be any number) is assigned to the leaves. By supposing that MIN plays optimally (i.e.&nbsp;in his own interest), it is possible to infer what the optimal strategy for MAX is..</p>
<p>The value of the leaves is propagated backwards to the starting position: MAX chooses the action leading to the state with the highest value, MIN does the opposite. MAX just has to play first the action with the highest value and let his opponent play. It might be necessary to re-plan if MIN is not optimal.</p>
<div id="fig-minimaxsol" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-minimaxsol-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/minimax-sol.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-minimaxsol-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Solution found by Minimax
</figcaption>
</figure>
</div>
<p>For most games, the tree becomes too huge for such a systematic search: The value of all states further than a couple of moves away are approximated by a <strong>heuristic function</strong>: the value <span class="math inline">\(V(s)\)</span> of these states. Obviously useless parts of the tree can be pruned: This is the <strong>Alpha-Beta</strong> algorithm. Alpha-Beta methods work well for simple problems where the complete game tree can be manipulated: Tic-Tac-Toe has only a couple of possible states and actions (<span class="math inline">\(3^9 = 19000\)</span> states). It also works when precise heuristics can be derived in a reasonable time. This is the principle of <strong>IBM DeepBlue</strong> which was the first Chess AI to beat a world champion (Garry Kasparov) in 1995. Carefully engineered heuristics (with the help of chess masters) allowed DeepBlue to search 6 moves away what is the best situation it can arrive in.</p>
<p>This approach does not work in Go because its branching factor (250 actions possible from each state) is too huge: the tree explodes very soon. <span class="math inline">\(250^{6} \approx 10^{15}\)</span>, so even if your processor evaluates 1 billion nodes per second, it would need 11 days to evaluate a single position 6 moves away…</p>
<div id="fig-gametreego" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gametreego-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/gotree.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gametreego-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Game tree of Go. Source: &lt;https://www.quora.com/What-does-it-mean-that-AlphaGo-relied-on-Monte Carlo-tree-search/answer/Kostis-Gourgoulias&gt;
</figcaption>
</figure>
</div>
</section>
<section id="alphago-1" class="level2">
<h2 class="anchored" data-anchor-id="alphago-1">AlphaGo</h2>
<section id="training-the-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="training-the-neural-networks">Training the neural networks</h3>
<p>AlphaGo <span class="citation" data-cites="Silver2016">[@Silver2016]</span> uses four different neural networks:</p>
<ul>
<li>The <strong>rollout policy</strong> and the <strong>SL policy network</strong> use supervised learning to predict expert human moves in any state.</li>
<li>The <strong>RL policy network</strong> uses <strong>self-play</strong> and reinforcement learning to learn new strategies.</li>
<li>The <strong>value network</strong> learns to predict the outcome of a game (win/lose) from the current state.</li>
</ul>
<p>The rollout policy and the value network are used to guide stochastic tree exploration in <strong>Monte Carlo Tree Search (MCTS)</strong> (MPC-like planning algorithm).</p>
<div id="fig-alphago-structure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alphago-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/alphago.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alphago-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Four neural networks are used in AlphaGo. Source: <span class="citation" data-cites="Silver2016">@Silver2016</span>
</figcaption>
</figure>
</div>
<p>Supervised learning is used for bootstrapping the <strong>policy network</strong> <span class="math inline">\(\rho_\sigma\)</span>, learning to predict human expert moves. 30M expert games have been gathered: the input is the 19x19 board configuration, the output is the move played by the expert. The CNN has 13 convolutional layers (5x5) and no max-pooling. The accuracy at the end of learning is 57% (not bad, but not sufficient to beat experts).</p>
<p>A faster <strong>rollout policy network</strong> <span class="math inline">\(\rho_\pi\)</span> is also trained: It has only one layer and views only part of the state (around the last opponent’s move). Its prediction accuracy is only 24%, but its inference time is only 2 <span class="math inline">\(\mu\)</span>s, instead of 3 ms for the policy network <span class="math inline">\(\rho_\sigma\)</span>. The idea of having a smaller rollout network has been reused later in I2A (see Section <strong>?@sec-i2a</strong>)</p>
<p>The SL policy network <span class="math inline">\(\rho_\sigma\)</span> is used to initialize the weights of the <strong>RL policy network</strong> <span class="math inline">\(\rho_\rho\)</span>, so it can start exploring from a decent policy. The RL policy network then plays against an <strong>older</strong> version of itself (<span class="math inline">\(\approx\)</span> target network) to improve its policy, updating the weights using <strong>Policy Gradient</strong> (REINFORCE):</p>
<p><span class="math display">\[
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, R ]
\]</span></p>
<p>where <span class="math inline">\(R\)</span> = +1 when the game is won, -1 otherwise.</p>
<p>The idea of playing against an older version of the same network (<strong>self-play</strong>) allows to learn offline, bypassing the need for (slow) human opponents. The RL policy network already wins 85% of the time against the strongest AI at the time (Pachi), but not against expert humans.</p>
<p>A <strong>value network</strong> <span class="math inline">\(\nu_\theta\)</span> finally learns to predict the outcome of a game (+1 when winning, -1 when losing) based on the self-play positions generated by the RL policy network.</p>
</section>
<section id="monte-carlo-tree-search" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-tree-search">Monte Carlo Tree Search</h3>
<div id="fig-mcts" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mcts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/mcts.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mcts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Monte Carlo Tree Search. Source: <span class="citation" data-cites="Silver2016">@Silver2016</span>
</figcaption>
</figure>
</div>
<p>The final AlphaGo player uses <strong>Monte Carlo Tree Search</strong> (MCTS), which is an incremental tree search (depth-limited), biased by the Q-value of known transitions. The game tree is traversed depth-first from the current state, but the order of the visits depends on the value of the transition. MCTS was previously the standard approach for Go AIs, but based on expert moves only, not deep networks.</p>
<p>In the <strong>selection phase</strong>, a path is found in the tree of possible actions using <strong>Upper Confidence Bound</strong> (UCB). The probability of selecting an action when sampling the tree depends on:</p>
<ul>
<li>Its Q-value <span class="math inline">\(Q(s, a)\)</span> (as learned by MCTS): how likely this action leads to winning.</li>
<li>Its prior probability: how often human players would play it, given by the SL policy network <span class="math inline">\(\rho_\sigma\)</span>.</li>
<li>Its number of visits <span class="math inline">\(N(s, a)\)</span>: this ensures exploration during the sampling.</li>
</ul>
<p><span class="math display">\[a_t = \text{argmax}_a \, Q(s, a) + K \cdot \frac{P(s, a)}{1 + N(s, a)}\]</span></p>
<p>In the <strong>expansion phase</strong>, a leaf state <span class="math inline">\(s_L\)</span> of the game tree is reached. The leaf is <strong>expanded</strong>, and the possible successors of that state are added to the tree. One requires a <strong>model</strong> to know which states are possible successors, but this is very easy in Go.</p>
<p><span class="math display">\[s_{t+1} = f(s_t, a_t)\]</span></p>
<p>The tree therefore grows every time a <strong>Monte Carlo sampling</strong> (“episode”) is done.</p>
<p>In the <strong>evaluation phase</strong>, the leaf <span class="math inline">\(s_L\)</span> is evaluated both by:</p>
<ul>
<li>the RL value network <span class="math inline">\(\nu_\theta\)</span> (how likely can we win from that state)</li>
<li>a random rollout until the end of the game using the fast rollout policy <span class="math inline">\(\rho_\pi\)</span>.</li>
</ul>
<p>The random rollout consists in “emulating” the end of the game using the fast rollout policy network. The rollout is of course imperfect, but complements the value network: they are more accurate together than alone!</p>
<p><span class="math display">\[V(s_L) = (1 - \lambda)  \, \nu_\theta(s_L) + \lambda \, R_\text{rollout} \]</span></p>
<p>This combination solves the bias/variance trade-off.</p>
<p>In the <strong>backup phase</strong>, the Q-values of all actions taken when descending the tree are updated with the value of the leaf node:</p>
<p><span class="math display">\[Q(s, a) = \frac{1}{N(s, a)} \sum_{i=1}^{n} V(s_L^i) \]</span></p>
<p>This is a Monte Carlo method: perform one episode and update the Q-value of all taken actions. However, it never uses real rewards, only value estimates. The Q-values are <strong>learned</strong> by using both the learned value of future states (value network) and internal simulations (rollout).</p>
<p>The four phases are then repeated as long as possible (time is limited in Go), to expand the game tree as efficiently as possible. The game tree is repeatedly sampled and grows after each sample. When the time is up, the greedy action (highest Q-value) in the initial state is chosen and played. For the next move, the tree is reset and expanded again (MPC replanning).</p>
<p>In the end, during MCTS, only the value network <span class="math inline">\(\nu_\theta\)</span>, the SL policy network <span class="math inline">\(\rho_\sigma\)</span> and the fast rollout policy <span class="math inline">\(\rho_\pi\)</span> are used. The RL policy network <span class="math inline">\(\rho_\rho\)</span> is only used to train the value network <span class="math inline">\(\nu_\theta\)</span>. i.e.&nbsp;to predict which positions are interesting or not. However, the RL policy network can discover new strategies by playing many times against itself, without relying on averaging expert moves like the previous approaches.</p>
<p>AlphaGo was able to beat Lee Sedol in 2016, 19 times World champion. It relies on human knowledge to <strong>bootstrap</strong> a RL agent (supervised learning). The RL agent discovers new strategies by using self-play: during the games against Lee Sedol, it was able to use <strong>novel</strong> moves which were never played before and surprised its opponent. The neural networks are only used to guide random search using MCTS: the policy network alone is not able to beat grandmasters. However, the computational cost of AlphaGo is huge: training took several weeks on 1202 CPUs and 176 GPUs…</p>
</section>
</section>
<section id="alphazero" class="level2">
<h2 class="anchored" data-anchor-id="alphazero">AlphaZero</h2>
<p>AlphaZero <span class="citation" data-cites="Silver2018">[@Silver2018]</span> totally skips the <strong>supervised learning</strong> part: the RL policy network starts self-play from scratch! The RL policy network uses MCTS to select moves, not a softmax-like selection as in AlphaGo. The policy and value networks are merged into a <strong>two-headed monster</strong>: the convolutional residual layers are shared to predict both:</p>
<ul>
<li>The policy <span class="math inline">\(\pi_\theta(s)\)</span>, which is only used to guide MCTS (prior of UCB).</li>
</ul>
<p><span class="math display">\[a_t = \text{argmax}_a \, Q(s, a) + K \cdot \frac{\pi_\theta(s, a)}{1 + N(s, a)}\]</span></p>
<ul>
<li>The state value <span class="math inline">\(V_\varphi(s)\)</span> for the value of the leaves (no fast rollout).</li>
</ul>
<div id="fig-alphazero" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alphazero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/alphazero.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alphazero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Architecture of AlphaZero. Source: <span class="citation" data-cites="Silver2018">@Silver2018</span>
</figcaption>
</figure>
</div>
<p>The loss function used to train the network is a <strong>compound loss</strong>:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = (R − V_\varphi(s))^2 - \pi_\text{MCTS}(s) \, \log \pi_\theta(s) + c ||\theta||^2
\]</span></p>
<p>The policy head <span class="math inline">\(\pi_\theta(s)\)</span> learns to mimic the actions selected by MCTS by minimizing the cross-entropy (or KL divergence): <strong>policy distillation</strong> (see Section <strong>?@sec-i2a</strong>). The value network <span class="math inline">\(V_\varphi(s)\)</span> learns to predict the return using Q-learning.</p>
<div id="fig-alphazeroperformance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alphazeroperformance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/AlphaZero-perf.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alphazeroperformance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Source: <a href="https://deepmind.com/blog/alphago-zero-learning-scratch/" class="uri">https://deepmind.com/blog/alphago-zero-learning-scratch/</a>
</figcaption>
</figure>
</div>
<p>By using a single network instead of four and learning faster, AlphaZero also greatly reduces the energy consumption.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/alphazero-efficiency.png" class="img-fluid figure-img"></p>
<figcaption>Source: <a href="https://deepmind.com/blog/alphago-zero-learning-scratch/" class="uri">https://deepmind.com/blog/alphago-zero-learning-scratch/</a></figcaption>
</figure>
</div>
<p>More impressively, <strong>the same algorithm can also play Chess and Shogi!</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/alphazero.gif" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Source: <a href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go" class="uri">https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go</a></figcaption>
</figure>
</div>
<p>The network weights are reset for each game, but it uses the same architecture and hyperparameters. After only 8 hours of training, AlphaZero beats Stockfish with 28-72-00, the best Chess AI at the time, which itself beats any human. This proves that the algorithm is generic and can be applied to any board game.</p>
</section>
<section id="muzero" class="level2">
<h2 class="anchored" data-anchor-id="muzero">MuZero</h2>
<p>MuZero <span class="citation" data-cites="Schrittwieser2019">[@Schrittwieser2019]</span>&nbsp;is the latest extension of AlphaZero (but see EfficientZero <span class="citation" data-cites="Ye2021">[@Ye2021]</span>). Instead of relying on a perfect simulator for the MCTS, it learns the dynamics model instead.</p>
<p><span class="math display">\[s_{t+1}, r_{t+1} = f_\theta(s_t, a_t)\]</span></p>
<div id="fig-muzero" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-muzero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/muzero.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-muzero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: MuZero. Source: <span class="citation" data-cites="Schrittwieser2019">@Schrittwieser2019</span>
</figcaption>
</figure>
</div>
<p>MuZero is composed of three neural networks:</p>
<ul>
<li>The representation network <span class="math inline">\(s= h(o_1, \ldots, o_t)\)</span> (encoder) transforming the history of observations into a state representation (<strong>latent space</strong>).</li>
<li>The dynamics model <span class="math inline">\(s', r = g(s, a)\)</span> used to generate rollouts for MCTS.</li>
<li>The policy and value network <span class="math inline">\(\pi, V = f(s)\)</span> learning the policy with PG.</li>
</ul>
<p>The dynamics model <span class="math inline">\(s', r = g(s, a)\)</span> replaces the perfect simulator in MCTS. It is used in the expansion phase of MCTS to add new nodes. Importantly, nodes are <strong>latent representations</strong> of the observations, not observations directly. This is a similar idea to <strong>World Models</strong> and <strong>PlaNet/Dreamer</strong>, which plan in the latent space of a VAE. Selection in MCTS still follows an upper confidence bound using the learned policy <span class="math inline">\(\pi\)</span>:</p>
<p><img src="img/muzero-ucb.png" class="img-fluid"></p>
<p>The actions taking during self-play are taken from the MCTS search as in AlphaZero. Note that the network plays each turn: there is additional information about whether the network is playing white or black. Self-played games are stored in a huge experience replay memory. Finally, complete games sampled from the ERM are used to learn simultaneously the three networks <span class="math inline">\(f\)</span>, <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span>.</p>
<p>MuZero beats AlphaZero on Chess, Go and Shogi, but also R2D2 on Atari games. The representation network <span class="math inline">\(h\)</span> allows to encode the Atari frames in a compressed manner that allows planning over raw images.</p>
<div id="fig-muzero-results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-muzero-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/muzero-results.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-muzero-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Results of MuZero. Source: <span class="citation" data-cites="Schrittwieser2019">@Schrittwieser2019</span>
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Additional resources
</div>
</div>
<div class="callout-body-container callout-body">
<p>A nice series of blog posts by David Foster explaining how to implement MuZero:</p>
<p><a href="https://medium.com/applied-data-science/how-to-build-your-own-muzero-in-python-f77d5718061a" class="uri">https://medium.com/applied-data-science/how-to-build-your-own-muzero-in-python-f77d5718061a</a></p>
<p><a href="https://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-2-3-f99dad7a7ad" class="uri">https://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-2-3-f99dad7a7ad</a></p>
<p><a href="https://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-3-3-ccea6b03538b" class="uri">https://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-3-3-ccea6b03538b</a></p>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>