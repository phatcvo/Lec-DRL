<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>entropyrl</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="2.8-EntropyRL_files/libs/clipboard/clipboard.min.js"></script>
<script src="2.8-EntropyRL_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="2.8-EntropyRL_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="2.8-EntropyRL_files/libs/quarto-html/popper.min.js"></script>
<script src="2.8-EntropyRL_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="2.8-EntropyRL_files/libs/quarto-html/anchor.min.js"></script>
<link href="2.8-EntropyRL_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="2.8-EntropyRL_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="2.8-EntropyRL_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="2.8-EntropyRL_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="2.8-EntropyRL_files/libs/bootstrap/bootstrap-bb8a42f168430693d1c0fc26666c4cdf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="sec-maxentrl" class="level1">
<h1>Maximum Entropy RL (SAC)</h1>
<p>All the methods seen sofar focus on finding a policy (or value functions) that maximizes the obtained return. This corresponds to the <strong>exploitation</strong> part of RL: we care only about the optimal policy. The <strong>exploration</strong> is ensured by external mechanisms, such as <span class="math inline">\(\epsilon\)</span>-greedy or softmax policies in value based methods, or adding exploratory noise to the actions as in DDPG. Exploration mechanisms typically add yet another free parameter (<span class="math inline">\(\epsilon\)</span>, softmax temperature, etc) that additionally need to be scheduled (more exploration at the beginning of learning than at the end).</p>
<p>The idea behind <strong>maximum entropy RL</strong> is to let the algorithm learn by itself how much exploration it needs to learn appropriately. There are several approaches to this problem (see for example <span class="citation" data-cites="Machado2018">[@Machado2018]</span> for an approach using successor representations), we focus first on methods using <strong>entropy regularization</strong>, a concept already seen briefly in A3C, before looking at soft methods such as Soft Q-learning and SAC.</p>
<section id="entropy-regularization" class="level2">
<h2 class="anchored" data-anchor-id="entropy-regularization">Entropy regularization</h2>
<p><strong>Entropy regularization</strong> <span class="citation" data-cites="Williams1991">[@Williams1991]</span> adds a regularization term to the objective function:</p>
<p><span class="math display">\[
    J(\theta) =  \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[ R(s_t, a_t) + \beta \,  H(\pi_\theta(s_t))]
\]</span></p>
<p>We will neglect here how the objective function is sampled (policy gradient, etc.) and focus on the second part.</p>
<p>The entropy of a discrete policy <span class="math inline">\(\pi_\theta\)</span> in a state <span class="math inline">\(s_t\)</span> is defined by the expected value of the <strong>self-information</strong> of each action :</p>
<p><span class="math display">\[
    H(\pi_\theta(s_t)) = \mathbb{E}_{a \sim \pi_\theta(s_t, a)} [- \log \pi_\theta(s_t, a)] = - \sum_a \pi_\theta(s_t, a) \, \log \pi_\theta(s_t, a)
\]</span></p>
<div id="fig-selfinformation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-selfinformation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/selfinformation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-selfinformation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The self-information <span class="math inline">\(- \log P(X=x)\)</span> of an outcome <span class="math inline">\(x\)</span> decreases with its probability: when an outcome is very likely (P close to 1), observing it is not surprising and does not provide much information. When an outcome is rare (P close to 0), observing it is very surprising and carries a lot of information.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Computing the entropy of a continuous policy distribution is more complex in the general case, but using a Gaussian policy (see Section <strong>?@sec-continuousspaces</strong>) allows to compute it easily. If the policy is a normal distribution <span class="math inline">\(\pi_\theta(s, a) = \mathcal{N}(\mu_\theta(s), \sigma^2_\theta(s))\)</span> parameterized by the mean vector <span class="math inline">\(\mu_\theta(s)\)</span> and the variance vector <span class="math inline">\(\sigma_\theta(s)\)</span> of size <span class="math inline">\(n\)</span>, the pdf is given by:</p>
<p><span class="math display">\[
    \pi_\theta(s, a) = \frac{1}{\sqrt{2\pi\sigma^2_\theta(s)}} \, \exp -\frac{(a - \mu_\theta(s))^2}{2\sigma_\theta(s)^2}
\]</span></p>
<p>The <strong>differential entropy</strong> of the Gaussian policy only depends on the variance of the distribution:</p>
<p><span class="math display">\[
    H(\pi_\theta(s_t)) = \mathbb{E}_{a \sim \pi_\theta(s_t, a)} [- \log \pi_\theta(s_t, a)] = \dfrac{n}{2} \, (1 + \ln 2\pi) + \dfrac{1}{2} \ln |\sigma_\theta(s)|
\]</span></p>
</div>
</div>
<p>The entropy of the policy measures its “randomness”:</p>
<ul>
<li>if the policy is fully deterministic (the same action is systematically selected), the entropy is zero as it carries no information.</li>
<li>if the policy is completely random (all actions are equally surprising), the entropy is maximal.</li>
</ul>
<p>By adding the entropy as a regularization term directly to the objective function, we force the policy to be as non-deterministic as possible, i.e.&nbsp;to explore as much as possible, while still getting as many rewards as possible. The parameter <span class="math inline">\(\beta\)</span> controls the level of regularization: we do not want the entropy to dominate either, as a purely random policy does not bring much reward. If <span class="math inline">\(\beta\)</span> is chosen too low, the entropy won’t play a significant role in the optimization and we may obtain a suboptimal deterministic policy early during training as there was not enough exploration. If <span class="math inline">\(\beta\)</span> is too high, the policy will be random and suboptimal.</p>
<p>Besides exploration, why would we want to learn a stochastic policy, while the solution to the Bellman equations is deterministic by definition? A first answer is that we rarely have a MDP: most interesting problems are POMDP, where the states are indirectly inferred through observations, which can be probabilistic. <span class="citation" data-cites="Todorov2008">@Todorov2008</span> showed that a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference <span class="citation" data-cites="Toussaint2009">[see also @Toussaint2009]</span>.</p>
<p>Consider a two-opponents game like chess: if you have a deterministic policy, you will always play the same moves in the same configuration. In particular, you will always play the same opening moves. Your game strategy becomes predictable for your opponent, who can adapt accordingly. Having a variety of opening moves (as long as they are not too stupid) is obviously a better strategy on the long term. This is due to the fact that chess is actually a POMDP: the opponent’s strategy and beliefs are not accessible.</p>
<p>Another way to view the interest of entropy regularization is to realize that learning a deterministic policy only leads to a single optimal solution to the problem. Learning a stochastic policy forces the agent to learn <strong>many</strong> optimal solutions to the same problem: the agent is somehow forced to learn as much information as possible for the experienced transitions, potentially reducing the sample complexity.</p>
<p>Entropy regularization is nowadays used very commonly used in deep RL networks <span class="citation" data-cites="ODonoghue2016">[e.g. @ODonoghue2016]</span>, as it is “only” an additional term to set in the objective function passed to the NN, adding a single hyperparameter <span class="math inline">\(\beta\)</span>.</p>
</section>
<section id="soft-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="soft-q-learning">Soft Q-learning</h2>
<p>Entropy regularization greedily maximizes the entropy of the policy in each state (the objective is the return plus the entropy in the current state). Building on the maximum entropy RL framework <span class="citation" data-cites="Ziebart2008 Schulman2017a Nachum2017">[@Ziebart2008;@Schulman2017a;@Nachum2017]</span>, <span class="citation" data-cites="Haarnoja2017">@Haarnoja2017</span> proposed a version of <strong>soft-Q-learning</strong> by extending the definition of the objective:</p>
<p><span class="math display">\[
    J(\theta) =  \sum_t \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[ r(s_t, a_t) + \beta \,  H(\pi_\theta(s_t))]
\]</span></p>
<p>In this formulation based on trajectories, the agent seeks a policy that maximizes the entropy of the complete trajectories rather than the entropy of the policy in each state. This is a very important distinction: the agent does not only search a policy with a high entropy, but a policy that brings into states with a high entropy, i.e.&nbsp;where the agent is the most uncertain. This allows for very efficient exploration strategies, where the agent will try to reduce its uncertainty about the world and gather a lot more information than when simply searching for a good policy.</p>
<p>Note that it is always possible to fall back to classical Q-learning by setting <span class="math inline">\(\beta=0\)</span> and that it is possible to omit this hyperparameter by scaling the rewards with <span class="math inline">\(\frac{1}{\beta}\)</span>. The discount rate <span class="math inline">\(\gamma\)</span> is omitted here for simplicity, but it should be added back when the task has an infinite horizon.</p>
<p>In soft Q-learning, the policy can be defined by a softmax over the soft Q-values <span class="math inline">\(Q_\text{soft}(s, a)\)</span>, where <span class="math inline">\(\beta\)</span> plays the role of the temperature parameter:</p>
<p><span class="math display">\[
    \pi(s, a) \propto \exp(Q_\text{soft}(s_t, a_t) / \beta)
\]</span></p>
<p>Note that <span class="math inline">\(-Q_\text{soft}(s_t, a_t) / \beta\)</span> plays the role of the energy of the policy (as in Boltzmann machines), hence the name of the paper (<em>Reinforcement Learning with Deep Energy-Based Policies</em>). We will ignore this analogy here. The normalization term of the softmax (the log-partition function in energy-based models) is also omitted as it later disappears from the equations anyway.</p>
<p>The soft Q-values are defined by the following Bellman-like equation:</p>
<p><span class="math display">\[
    Q_\text{soft}(s_t, a_t) = r(s_t, a_t) + \gamma \, \mathbb{E}_{s_{t+1} \in \rho} [V_\text{soft}(s_{t+1})]
\]</span></p>
<p>This is the regular Bellman equation that can be turned into an update rule for the soft Q-values (minimizing the mse between the l.h.s and the r.h.s). The soft value of a state is given by:</p>
<p><span class="math display">\[
    V_\text{soft}(s_t) = \mathbb{E}_{a_{t} \in \pi} [Q_\text{soft}(s_{t}, a_{t}) - \log \, \pi(s_t, a_t)]
\]</span></p>
<p>The notation in <span class="citation" data-cites="Haarnoja2017">@Haarnoja2017</span> is much more complex than that (the paper includes the theoretical proofs), but it boils down to this in <span class="citation" data-cites="Haarnoja2018a">@Haarnoja2018a</span>. When definition of the soft Q-value is applied repeatedly with the definition of the soft V-value, it converges to the optimal solution of the soft Q-learning objective, at least in the tabular case.</p>
<p>The soft V-value of a state is the expectation of the Q-values in that state (as in regular RL) minus the log probability of each action. This last term measures the entropy of the policy in each state (when expanding the expectation over the policy, we obtain <span class="math inline">\(- \pi \log \pi\)</span>, which is the entropy).</p>
<p>In a nutshell, the soft Q-learning algorithm is:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Soft Q-learning
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Sample transitions <span class="math inline">\((s, a, r, s')\)</span> and store them in a replay memory.</p></li>
<li><p>For each transition <span class="math inline">\((s, a, r, s')\)</span> in a minibatch of the replay memory:</p>
<ul>
<li><p>Estimate <span class="math inline">\(V_\text{soft}(s')\)</span> by sampling several actions: <span class="math display">\[
  V_\text{soft}(s_t) = \mathbb{E}_{a_{t} \in \pi} [Q_\text{soft}(s_{t}, a_{t}) - \log \, \pi(s_t, a_t)]
  \]</span></p></li>
<li><p>Update the soft Q-value of <span class="math inline">\((s,a)\)</span>: <span class="math display">\[
  Q_\text{soft}(s_t, a_t) = r(s_t, a_t) + \gamma \, \mathbb{E}_{s_{t+1} \in \rho} [V_\text{soft}(s_{t+1})]
  \]</span></p></li>
<li><p>Update the policy (if not using the softmax over soft Q-values directly).</p></li>
</ul></li>
</ul>
</div>
</div>
<p>The main drawback of this approach is that several actions have to be sampled in the next state in order to estimate its current soft V-value, what makes it hard to implement in practice. The policy also has to be sampled from the Q-values, what is not practical for continuous action spaces.</p>
<p>But the real interesting thing is the policies that are learned in multi-goal settings, as in <a href="#fig-softql" class="quarto-xref">Figure&nbsp;2</a>. The agent starts in the middle of the environment and can obtain one of the four rewards (north, south, west, east). A regular RL agent would very quickly select only one of the rewards and stick to it. With soft Q-learning, the policy stays stochastic and the four rewards can be obtained even after convergence. This indicates that the soft agent has learned much more about its environment than its hard equivalent, thanks to its maximum entropy formulation.</p>
<div id="fig-softql" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-softql-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/softQL.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-softql-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Policy learned by Soft Q-learning in a multi-goal setting. Source: <span class="citation" data-cites="Haarnoja2017">@Haarnoja2017</span>.
</figcaption>
</figure>
</div>
</section>
<section id="soft-actor-critic-sac" class="level2">
<h2 class="anchored" data-anchor-id="soft-actor-critic-sac">Soft Actor-Critic (SAC)</h2>
<p><span class="citation" data-cites="Haarnoja2018a">@Haarnoja2018a</span> proposed the <strong>Soft Actor-Critic</strong> (SAC), an off-policy actor-critic which allows to have a stochastic actor (contrary to DDPG) while being more optimal and sample efficient than on-policy methods such as A3C or PPO. It is also less sensible to hyperparameters than all these methods.</p>
<p>SAC builds on soft Q-learning to achieve these improvements. It relies on three different function approximators:</p>
<ul>
<li>a soft state value function <span class="math inline">\(V_\varphi(s)\)</span>.</li>
<li>a soft Q-value function <span class="math inline">\(Q_\psi(s,a)\)</span>.</li>
<li>a stochastic policy <span class="math inline">\(\pi_\theta(s, a)\)</span>.</li>
</ul>
<p>The paper uses a different notation for the parameters <span class="math inline">\(\theta, \varphi, \psi\)</span>, but I choose to be consistent with the rest of this document.</p>
<p>The soft state-value function <span class="math inline">\(V_\varphi(s)\)</span> is learned vy turning the definition of the soft V-value into a loss function:</p>
<p><span class="math display">\[
    \mathcal{L}(\varphi) = \mathbb{E}_{s_t \in \mathcal{D}} [\mathbb{E}_{a_{t} \in \pi} [(Q_\psi(s_{t}, a_{t}) - \log \, \pi_\theta(s_t, a_t)] - V_\varphi(s_t) )^2]
\]</span></p>
<p>In practice, we only need the gradient of this loss function to train the corresponding neural network. The expectation over the policy inside the loss function can be replaced by a single sample action <span class="math inline">\(a\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span> (but not <span class="math inline">\(a_{t+1}\)</span> in the replay memory <span class="math inline">\(\mathcal{D}\)</span>, which is only used for the states <span class="math inline">\(s_t\)</span>).</p>
<p><span class="math display">\[
    \nabla_\varphi \mathcal{L}(\varphi) = \nabla_\varphi V_\varphi(s_t) \, (V_\varphi(s_t) - Q_\psi(s_{t}, a) + \log \, \pi_\theta(s_t, a) )
\]</span></p>
<p>The soft Q-values <span class="math inline">\(Q_\psi(s_{t}, a_{t})\)</span> can be trained from the replay memory <span class="math inline">\(\mathcal{D}\)</span> on <span class="math inline">\((s_t, a_t, r_{t+1} , s_{t+1})\)</span> transitions by minimizing the mse:</p>
<p><span class="math display">\[
    \mathcal{L}(\psi) = \mathbb{E}_{s_t, a_t \in \mathcal{D}} [(r_{t+1} + \gamma \, V_\varphi(s_{t+1}) - Q_\psi(s_t, a_t))^2]
\]</span></p>
<p>Finally, the policy <span class="math inline">\(\pi_\theta\)</span> can be trained to maximize the obtained returns. There are many ways to do that, but <span class="citation" data-cites="Haarnoja2018a">@Haarnoja2018a</span> proposes to minimize the Kullback-Leibler (KL) divergence between the current policy <span class="math inline">\(\pi_\theta\)</span> and a softmax function over the soft Q-values:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_{s_t \in \mathcal{D}} [D_\text{KL}(\pi_\theta(s, \cdot) | \frac{\exp Q_\psi(s_t, \cdot)}{Z(s_t)})]
\]</span></p>
<p>where <span class="math inline">\(Z\)</span> is the partition function to normalize the softmax. Fortunately, it disappears when using the reparameterization trick and taking the gradient of this loss (see the paper for details).</p>
<p>There are additional tricks to make it more efficient and robust, such as target networks or the use of two independent function approximators for the soft Q-values in order to reduce the bias, but the gist of the algorithm is the following:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SAC algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Sample a transition <span class="math inline">\((s_t, a_t, r_{t+1}, a_{t+1})\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span> and store it in the replay memory <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p>For each transition <span class="math inline">\((s_t, a_t, r_{t+1}, a_{t+1})\)</span> of a minibatch of <span class="math inline">\(\mathcal{D}\)</span>:</p>
<ul>
<li><p>Sample an action <span class="math inline">\(a \in \pi_\theta(s_t, \cdot)\)</span> from the current policy.</p></li>
<li><p>Update the soft state-value function <span class="math inline">\(V_\varphi(s_t)\)</span>:</p></li>
</ul>
<p><span class="math display">\[
      \nabla_\varphi \mathcal{L}(\varphi) = \nabla_\varphi V_\varphi(s_t) \, (V_\varphi(s_t) - Q_\psi(s_{t}, a) + \log \, \pi_\theta(s_t, a) )
  \]</span></p>
<ul>
<li>Update the soft Q-value function <span class="math inline">\(Q_\psi(s_t, a_t)\)</span>:</li>
</ul>
<p><span class="math display">\[
      \nabla_\psi \mathcal{L}(\psi) = - \nabla_\psi Q_\psi(s_t, a_t) \, (r_{t+1} + \gamma \, V_\varphi(s_{t+1}) - Q_\psi(s_t, a_t))
  \]</span></p>
<ul>
<li>Update the policy <span class="math inline">\(\pi_\theta(s_t, \cdot)\)</span>:</li>
</ul>
<p><span class="math display">\[
      \nabla_\theta \mathcal{L}(\theta) = \nabla_\theta D_\text{KL}(\pi_\theta(s, \cdot) | \frac{\exp Q_\psi(s_t, \cdot)}{Z(s_t)})
  \]</span></p></li>
</ul>
</div>
</div>
<p>SAC was compared to DDPG, PPO, soft Q-learning and others on a set of gym and humanoid robotics tasks (with 21 joints!). It outperforms all these methods in both the final performance and the sample complexity, the difference being even more obvious for the complex tasks. The exploration bonus given by the maximum entropy allows the agent to discover better policies than its counterparts. SAC is an actor-critic architecture (the critic computing both V and Q) working off-policy (using an experience replay memory, so re-using past experiences) allowing to learn stochastic policies, even in high dimensional spaces.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>