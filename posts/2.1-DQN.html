<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>dqn</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="2.1-DQN_files/libs/clipboard/clipboard.min.js"></script>
<script src="2.1-DQN_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="2.1-DQN_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="2.1-DQN_files/libs/quarto-html/popper.min.js"></script>
<script src="2.1-DQN_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="2.1-DQN_files/libs/quarto-html/anchor.min.js"></script>
<link href="2.1-DQN_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="2.1-DQN_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="2.1-DQN_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="2.1-DQN_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="2.1-DQN_files/libs/bootstrap/bootstrap-bb8a42f168430693d1c0fc26666c4cdf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="deep-q-network-dqn" class="level1">
<h1>Deep Q-network (DQN)</h1>
<section id="limitations-of-deep-neural-networks-for-function-approximation" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-deep-neural-networks-for-function-approximation">Limitations of deep neural networks for function approximation</h2>
<p>The goal of value-based deep RL is to approximate the Q-value of each possible state-action pair using a deep neural network. As shown on <a href="#fig-functionapprox" class="quarto-xref">Figure&nbsp;1</a>, the network can either take a state-action pair as input and return a single output value, or take only the state as input and return the Q-value of all possible actions (only possible if the action space is discrete). In both cases, the goal is to learn estimates <span class="math inline">\(Q_\theta(s, a)\)</span> with a NN with parameters <span class="math inline">\(\theta\)</span>.</p>
<div id="fig-functionapprox" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-functionapprox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/functionapprox.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-functionapprox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Function approximators can either associate a state-action pair <span class="math inline">\((s, a)\)</span> to its Q-value (left), or associate a state <span class="math inline">\(s\)</span> to the Q-values of all actions possible in that state (right).
</figcaption>
</figure>
</div>
<p>When using Q-learning, we have already seen that the problem is a regression problem, where the following mse loss function has to be minimized:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_{(s, a, r ,s')}[(r(s, a, s') + \gamma \, \max_{a'} Q_\theta(s', a') - Q_\theta(s, a))^2]
\]</span></p>
<p>In short, we want to reduce the prediction error, i.e.&nbsp;the mismatch between the estimate of the value of an action <span class="math inline">\(Q_\theta(s, a)\)</span> and the real return <span class="math inline">\(Q^\pi(s, a)\)</span>, here approximated with <span class="math inline">\(r(s, a, s') + \gamma \, \text{max}_{a'} Q_\theta(s', a')\)</span>.</p>
<div id="fig-valuebasedagent" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-valuebasedagent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/valuebased-agent.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-valuebasedagent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Value-based Q-learning agent.
</figcaption>
</figure>
</div>
<p>We can compute this loss by gathering enough samples <span class="math inline">\((s, a, r, s')\)</span> (i.e.&nbsp;single transitions), concatenating them randomly in minibatches, and let the DNN learn to minimize the prediction error using backpropagation and SGD, indirectly improving the policy. The following pseudocode would describe the training procedure when gathering transitions <strong>online</strong>, i.e.&nbsp;when directly interacting with the environment:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Naive Q-learning with function approximation
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Initialize value network <span class="math inline">\(Q_{\theta}\)</span> with random weights.</p></li>
<li><p>Initialize empty minibatch <span class="math inline">\(\mathcal{D}\)</span> of maximal size <span class="math inline">\(n\)</span>.</p></li>
<li><p>Observe the initial state <span class="math inline">\(s_0\)</span>.</p></li>
<li><p>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:</p>
<ul>
<li><p>Select the action <span class="math inline">\(a_t\)</span> based on the behavior policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g.&nbsp;softmax).</p></li>
<li><p>Perform the action <span class="math inline">\(a_t\)</span> and observe the next state <span class="math inline">\(s_{t+1}\)</span> and the reward <span class="math inline">\(r_{t+1}\)</span>.</p></li>
<li><p>Store <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in the minibatch.</p></li>
<li><p>When minibatch <span class="math inline">\(\mathcal{D}\)</span> is full:</p>
<ul>
<li>Train the value network <span class="math inline">\(Q_{\theta}\)</span> on <span class="math inline">\(\mathcal{D}\)</span> to minimize</li>
</ul>
<p><span class="math display">\[
  \mathcal{L}(\theta) = \mathbb{E}_\mathcal{D}[(r(s, a, s') + \gamma \, \text{max}_{a'} Q_\theta(s', a') - Q_\theta(s, a))^2]
  \]</span></p>
<ul>
<li>Empty the minibatch <span class="math inline">\(\mathcal{D}\)</span>.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<p>However, the definition of the loss function uses the mathematical expectation operator <span class="math inline">\(E\)</span> over all transitions, which can only be approximated by <strong>randomly</strong> sampling the distribution (the MDP). This implies that the samples concatenated in a minibatch should be independent from each other (<strong>i.i.d</strong>).</p>
<section id="correlated-inputs" class="level3">
<h3 class="anchored" data-anchor-id="correlated-inputs">Correlated inputs</h3>
<p>When gathering transitions online, the samples are correlated: <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> will be followed by <span class="math inline">\((s_{t+1}, a_{t+1}, r_{t+2}, s_{t+2})\)</span>, etc. When playing video games, two successive frames will be very similar (a few pixels will change, or even none if the sampling rate is too high) and the optimal action will likely not change either (to catch the ball in pong, you will need to perform the same action - going left - many times in a row).</p>
<div id="fig-breakout-correlated" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-breakout-correlated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/breakout.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-breakout-correlated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Sucessive frames in a video game are highly correlated.
</figcaption>
</figure>
</div>
<p><strong>Correlated inputs/outputs</strong> are very bad for deep neural networks: the DNN will overfit and fall into a very bad local minimum. That is why stochastic gradient descent works so well: it randomly samples values from the training set to form minibatches and minimize the loss function on these uncorrelated samples (hopefully). If all samples of a minibatch were of the same class (e.g.&nbsp;zeros in MNIST), the network would converge poorly. This is the first problem preventing an easy use of deep neural networks as function approximators in RL.</p>
</section>
<section id="non-stationary-targets" class="level3">
<h3 class="anchored" data-anchor-id="non-stationary-targets">Non-stationary targets</h3>
<p>The second major problem is the <strong>non-stationarity</strong> of the targets in the loss function. In classification or regression, the desired values <span class="math inline">\(\mathbf{t}\)</span> are fixed throughout learning: the class of an object does not change in the middle of the training phase.</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = - \mathbb{E}_{\mathbf{x}, \mathbf{t} \in \mathcal{D}}[ ||\mathbf{t} - \mathbf{y}||^2]
\]</span></p>
<p>In Q-learning, the target :</p>
<p><span class="math display">\[
    t = r(s, a, s') + \gamma \, \max_{a'} Q_\theta(s', a')
\]</span></p>
<p>will change during learning, as <span class="math inline">\(Q_\theta(s', a')\)</span> depends on the weights <span class="math inline">\(\theta\)</span> and will hopefully increase as the performance improves. This is the second problem of deep RL: deep NN are particularly bad on non-stationary problems, especially feedforward networks. They iteratively converge towards the desired value, but have troubles when the target also moves (like a dog chasing its tail).</p>
<div id="fig-nonstationarity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonstationarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/nonstationarity.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nonstationarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: In supervised learning, the targets are stationary, leading to good convergence properties. In RL, the targets are non-stationary and depending on the network itself. This often leads to suboptimal convergence.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="deep-q-network-dqn-1" class="level2">
<h2 class="anchored" data-anchor-id="deep-q-network-dqn-1">Deep Q-Network (DQN)</h2>
<p><span class="citation" data-cites="Mnih2015">@Mnih2015</span> (originally arXived in <span class="citation" data-cites="Mnih2013">@Mnih2013</span>) proposed an elegant solution to the problems of correlated inputs/outputs and non-stationarity inherent to RL. This article is a milestone of deep RL and it is fair to say that it started the hype for deep RL.</p>
<section id="experience-replay-memory" class="level3">
<h3 class="anchored" data-anchor-id="experience-replay-memory">Experience replay memory</h3>
<p>The first idea proposed by <span class="citation" data-cites="Mnih2015">@Mnih2015</span> solves the problem of correlated input/outputs and is actually quite simple: instead of feeding successive transitions into a minibatch and immediately training the NN on it, transitions are stored in a huge buffer called <strong>experience replay memory</strong> (ERM) or <strong>replay buffer</strong> able to store 100000 transitions. When the buffer is full, new transitions replace the old ones. SGD can now randomly sample the ERM to form minibatches and train the NN.</p>
<div id="fig-erm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-erm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/ERM.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-erm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Experience replay memory. Interactions with the environment are stored in the ERM. Random minibatches are sampled from it to train the DQN value network.
</figcaption>
</figure>
</div>
<p>The loss minimized by DQN is defined on a minibatch of size <span class="math inline">\(K\)</span>:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \dfrac{1}{K} \, \sum_{k=1}^K (r_k + \gamma \, \text{max}_{a'} Q_\theta(s'_k, a') - Q_\theta(s_k, a_k))^2
\]</span></p>
<p>Are these <span class="math inline">\(K\)</span> samples <strong>i.i.d</strong>? They are independent because they are randomly sampled from the ERM, but they do not come from the same distribution: some were generated by a very old policy, some much more recently… However, this does not matter, as Q-learning is <strong>off-policy</strong>: the different policies that populated the ERM are a <strong>behavior policy</strong>, different from the learned one. Off-policy methods do not mind if the samples come from the same distribution or not. It would be very different if we has used SARSA instead.</p>
<p>→ <strong>It is only possible to use an experience replay memory with off-policy algorithms</strong></p>
</section>
<section id="target-networks" class="level3">
<h3 class="anchored" data-anchor-id="target-networks">Target networks</h3>
<p>The second idea solves the non-stationarity of the targets <span class="math inline">\(r(s, a, s') + \gamma \, \max_{a'} Q_\theta(s', a')\)</span>. Instead of computing it with the current parameters <span class="math inline">\(\theta\)</span> of the NN, they are computed with an old version of the NN called the <strong>target network</strong> with parameters <span class="math inline">\(\theta'\)</span>.</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \dfrac{1}{K} \, \sum_{k=1}^K (r_k + \gamma \, \text{max}_{a'} Q_{\theta'}(s'_k, a') - Q_\theta(s_k, a_k))^2
\]</span></p>
<p>The target network is updated only infrequently (every thousands of iterations or so) with the learned weights <span class="math inline">\(\theta\)</span>. As this target network does not change very often, the targets stay constant for a long period of time, and the problem becomes more stationary.</p>
<div id="fig-targetnetwork" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-targetnetwork-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/targetnetwork.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-targetnetwork-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: The target network is used to compute the targets to train the value network. Its waits are regularly copied from the value network.
</figcaption>
</figure>
</div>
<div id="fig-nonstationarity2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonstationarity2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/nonstationarity2.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nonstationarity2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: By keeping the the targets constant for a while, the target network lets the value network catch up with them and converge optimally (in principle).
</figcaption>
</figure>
</div>
</section>
<section id="dqn-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="dqn-algorithm">DQN algorithm</h3>
<p>The resulting algorithm is called <strong>Deep Q-Network (DQN)</strong>. It is summarized by the following pseudocode:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
DQN algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Initialize value network <span class="math inline">\(Q_{\theta}\)</span> with random weights.</p></li>
<li><p>Copy <span class="math inline">\(Q_{\theta}\)</span> to create the target network <span class="math inline">\(Q_{\theta'}\)</span>.</p></li>
<li><p>Initialize experience replay memory <span class="math inline">\(\mathcal{D}\)</span> of maximal size <span class="math inline">\(N\)</span>.</p></li>
<li><p>Observe the initial state <span class="math inline">\(s_0\)</span>.</p></li>
<li><p>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:</p>
<ul>
<li><p>Select the action <span class="math inline">\(a_t\)</span> based on the behavior policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g.&nbsp;softmax).</p></li>
<li><p>Perform the action <span class="math inline">\(a_t\)</span> and observe the next state <span class="math inline">\(s_{t+1}\)</span> and the reward <span class="math inline">\(r_{t+1}\)</span>.</p></li>
<li><p>Store <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in the experience replay memory.</p></li>
<li><p>Every <span class="math inline">\(T_\text{train}\)</span> steps:</p>
<ul>
<li><p>Sample a minibatch <span class="math inline">\(\mathcal{D}_s\)</span> randomly from <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p>For each transition <span class="math inline">\((s, a, r, s')\)</span> in the minibatch:</p>
<ul>
<li><p>Predict the Q-value of the greedy action in the next state <span class="math inline">\(\max_{a'} Q_{\theta'}(s', a')\)</span> using the target network.</p></li>
<li><p>Compute the target value <span class="math inline">\(t = r + \gamma \, \max_{a'} Q_{\theta'}(s', a')\)</span>.</p></li>
</ul></li>
<li><p>Train the value network <span class="math inline">\(Q_{\theta}\)</span> on <span class="math inline">\(\mathcal{D}_s\)</span> to minimize <span class="math inline">\(\mathcal{L}(\theta) = \mathbb{E}_{\mathcal{D}_s}[(t - Q_\theta(s, a))^2]\)</span></p></li>
</ul></li>
<li><p>Every <span class="math inline">\(T_\text{target}\)</span> steps:</p>
<ul>
<li>Update the target network with the trained value network: <span class="math inline">\(\theta' \leftarrow \theta\)</span></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<p>The first thing to notice is that experienced transitions are not immediately used for learning, but simply stored in the ERM to be sampled later. Due to the huge size of the ERM, it is even likely that the recently experienced transition will only be used for learning hundreds or thousands of steps later. Meanwhile, very old transitions, generated using an initially bad policy, can be used to train the network for a very long time.</p>
<p>The second thing is that the target network is not updated very often (<span class="math inline">\(T_\text{target}=10000\)</span>), so the target values are going to be wrong a long time. More recent algorithms such as DDPG use a smoothed version of the current weights, as proposed in <span class="citation" data-cites="Lillicrap2015">@Lillicrap2015</span>:</p>
<p><span class="math display">\[
    \theta' = \tau \, \theta + (1-\tau) \, \theta'
\]</span></p>
<p>If this rule is applied after each step with a very small rate <span class="math inline">\(\tau\)</span>, the target network will slowly track the learned network, but never be the same. Modern implementations of DQN use this smoothed version.</p>
<p>These two facts make DQN extremely slow to learn: millions of transitions are needed to obtain a satisfying policy. This is called the <strong>sample complexity</strong>, i.e.&nbsp;the number of transitions needed to obtain a satisfying performance. DQN finds very good policies, but at the cost of a very long training time.</p>
<p>DQN was initially applied to solve various Atari 2600 games. Video frames were used as observations and the set of possible discrete actions was limited (left/right/up/down, shoot, etc). The CNN used is depicted on <a href="#fig-dqn" class="quarto-xref">Figure&nbsp;8</a>. It has two convolutional layers, no max-pooling, 2 fully-connected layer and one output layer representing the Q-value of all possible actions in the games.</p>
<div id="fig-dqn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dqn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/dqn.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dqn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Architecture of the CNN used in the original DQN paper. Source: <span class="citation" data-cites="Mnih2015">@Mnih2015</span>.
</figcaption>
</figure>
</div>
<p>The problem of partial observability (a single frame does not hold the Markov property) is solved by concatenating the four last video frames into a single tensor used as input to the CNN. The convolutional layers become able through learning to extract the speed information from it. Some of the Atari games (Pinball, Breakout) were solved with a performance well above human level, especially when they are mostly reactive. Games necessitating more long-term planning (Montezuma’s Revenge) were still poorly learned, though.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why no max-pooling?
</div>
</div>
<div class="callout-body-container callout-body">
<p>The CNN used in deep RL agents (DQN or others) usually do not have many max-pooling layers (or strides, which are equivalent). The goal of a max-pooling layer is to achieve <strong>spatial invariance</strong>, i.e.&nbsp;being able to recognize an object whatever its position in the input image. A cat is a cat, whether it is on the left or the right of the image.</p>
<p>However, we usually do not want spatial invariance in RL: the location of the ball in the frame in Breakout or Pinball is extremely important for the policy, we do not want to get rid of it.</p>
<p>The drawback of not having max-pooling layers is that the last convolutional layer (before the first FC layer) will still have a lot of elements, so the first FC matrix will likely be huge. This limits the ability of deep RL algorithms to work with big images.</p>
</div>
</div>
<p>Beside being able to learn using delayed and sparse rewards in highly dimensional input spaces, the true <em>tour de force</em> of DQN is that it was able to learn the 49 Atari games using the same architecture and hyperparameters, showing the generality of the approach.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/TmPfTpjtdgg" title="DQN" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/W2CAghUiofY" title="DQN" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div id="fig-dqnresults" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dqnresults-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/atari-results.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dqnresults-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Results on the Atari benchmark. Some games achieved super-human performance. Source: <span class="citation" data-cites="Mnih2015">@Mnih2015</span>
</figcaption>
</figure>
</div>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>