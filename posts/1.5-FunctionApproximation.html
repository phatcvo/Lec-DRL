<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>functionapproximation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="1.5-FunctionApproximation_files/libs/clipboard/clipboard.min.js"></script>
<script src="1.5-FunctionApproximation_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="1.5-FunctionApproximation_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="1.5-FunctionApproximation_files/libs/quarto-html/popper.min.js"></script>
<script src="1.5-FunctionApproximation_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="1.5-FunctionApproximation_files/libs/quarto-html/anchor.min.js"></script>
<link href="1.5-FunctionApproximation_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="1.5-FunctionApproximation_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="1.5-FunctionApproximation_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="1.5-FunctionApproximation_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="1.5-FunctionApproximation_files/libs/bootstrap/bootstrap-bb8a42f168430693d1c0fc26666c4cdf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="function-approximation" class="level1">
<h1>Function approximation</h1>
<p>All the methods presented before are <em>tabular methods</em>, as one needs to store one value per state-action pair: either the Q-value of the action or a preference for that action.</p>
<div id="fig-qtable" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qtable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/qtable.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-qtable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Source: <a href="https://towardsdatascience.com/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677" class="uri">https://towardsdatascience.com/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677</a>
</figcaption>
</figure>
</div>
<p>In most useful applications, the number of values to store would quickly become prohibitive: when working on raw images, the number of possible states alone is untractable. Moreover, these algorithms require that each state-action pair is visited a sufficient number of times to converge towards the optimal policy: if a single state-action pair is never visited, there is no guarantee that the optimal policy will be found. The problem becomes even more obvious when considering <em>continuous</em> state or action spaces.</p>
<p>However, in a lot of applications, the optimal action to perform in two very close states is likely to be the same: changing one pixel in a video game does not change which action should be applied. It would therefore be very useful to be able to <strong>interpolate</strong> Q-values between different states: only a subset of all state-action pairs has to explored; the others will be “guessed” depending on the proximity between the states and/or the actions. The problem is now <strong>generalization</strong>, i.e.&nbsp;transferring acquired knowledge to unseen but similar situations.</p>
<div id="fig-generalization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-generalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/tabular-generalization.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-generalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Generalization between similar states is not possible in tabular RL.
</figcaption>
</figure>
</div>
<p>This is where <strong>function approximation</strong> (FA) becomes useful: the V/Q-values or the policy are not stored in a table, but rather learned by a function approximator. The type of function approximator does not really matter here: in deep RL we are of course interested in deep neural networks, but any kind of regressor theoretically works (linear algorithms, radial-basis function network, SVR…).</p>
<section id="value-based-function-approximation" class="level2">
<h2 class="anchored" data-anchor-id="value-based-function-approximation">Value-based function approximation</h2>
<section id="state-value-approximators" class="level3">
<h3 class="anchored" data-anchor-id="state-value-approximators">State value approximators</h3>
<p>Let’s represent a state <span class="math inline">\(s\)</span> by a vector of <span class="math inline">\(d\)</span> <strong>features</strong> <span class="math inline">\(\phi(s) = [\phi_1(s), \phi_2(s), \ldots, \phi_d(s)]^T\)</span>. For the cartpole, the feature vector would be:</p>
<p><span class="math display">\[ \phi(s) = \begin{bmatrix}x \\ \dot{x} \\ \theta \\ \dot{\theta} \end{bmatrix}\]</span></p>
<p><span class="math inline">\(x\)</span> is the position, <span class="math inline">\(\theta\)</span> the angle, <span class="math inline">\(\dot{x}\)</span> and <span class="math inline">\(\dot{\theta}\)</span> their derivatives. We are able to represent <strong>any state</strong> <span class="math inline">\(s\)</span> of the Cartpole using these four variables. If the state can be represented by an image, we only need to put its pixels into a single vector. For more complex problems, the feature vector should include all the necessary information (Markov property).</p>
<p>In <strong>state value approximation</strong>, we want to approximate the state value function <span class="math inline">\(V^\pi(s)\)</span> with a <strong>parameterized function</strong> <span class="math inline">\(V_\varphi(s)\)</span>:</p>
<p><span class="math display">\[V_\varphi(s) \approx V^\pi(s)\]</span></p>
<div id="fig-FA-V" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-FA-V-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/functionapproximation-state.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-FA-V-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Parameterized model to approximate state values.
</figcaption>
</figure>
</div>
<p>The parameterized function can have any form. It has a set of parameters <span class="math inline">\(\varphi\)</span> used to transform the feature vector <span class="math inline">\(\phi(s)\)</span> into an approximated value <span class="math inline">\(V_\varphi(s)\)</span>.</p>
<p>The simplest function approximator (FA) is the <strong>linear approximator</strong>.</p>
<div id="fig-FA-Vlinear" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-FA-Vlinear-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/functionapproximation-state-linear.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-FA-Vlinear-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Linear approximator for state values.
</figcaption>
</figure>
</div>
<p>The approximated value is a linear combination of the features:</p>
<p><span class="math display">\[V_\varphi(s) = \sum_{i=1}^d w_i \, \phi_i(s) = \mathbf{w}^T \times \phi(s)\]</span></p>
<p>The <strong>weight vector</strong> <span class="math inline">\(\mathbf{w} = [w_1, w_2, \ldots, w_d]^T\)</span>is the set of parameters <span class="math inline">\(\varphi\)</span> of the function.</p>
<p>Regardless the form of the function approximator, we want to find the parameters <span class="math inline">\(\varphi\)</span> making the approximated values <span class="math inline">\(V_\varphi(s)\)</span> as close as possible from the true values <span class="math inline">\(V^\pi(s)\)</span> for all states <span class="math inline">\(s\)</span>. This is a <strong>regression</strong> problem. We want to minimize the <strong>mean square error</strong> between the two quantities:</p>
<p><span class="math display">\[ \min_\varphi \mathcal{L}(\varphi) = \mathbb{E}_{s \in \mathcal{S}} [ (V^\pi(s) - V_\varphi(s))^2]\]</span></p>
<p>The <strong>loss function</strong> <span class="math inline">\(\mathcal{L}(\varphi)\)</span> is minimal when the predicted values are close to the true ones on average for all states. Let’s suppose that we know the true state values <span class="math inline">\(V^\pi(s)\)</span> for all states and that the parameterized function is <strong>differentiable</strong>. We can find the minimum of the loss function by applying <strong>gradient descent</strong> (GD) iteratively:</p>
<p><span class="math display">\[
    \Delta \varphi = - \eta \, \nabla_\varphi \mathcal{L}(\varphi)
\]</span></p>
<p><span class="math inline">\(\nabla_\varphi \mathcal{L}(\varphi)\)</span> is the gradient of the loss function w.r.t to the parameters <span class="math inline">\(\varphi\)</span>:</p>
<p><span class="math display">\[
    \nabla_\varphi \mathcal{L}(\varphi) = \begin{bmatrix}
        \dfrac{\partial \mathcal{L}(\varphi)}{\partial \varphi_1} \\
        \dfrac{\partial \mathcal{L}(\varphi)}{\partial \varphi_2} \\
        \ldots \\
        \dfrac{\partial \mathcal{L}(\varphi)}{\partial \varphi_K} \\
    \end{bmatrix}
\]</span></p>
<p>When applied repeatedly, GD converges to a local minimum of the loss function. To minimize the mean square error, we just need to compute its gradient with respect to the parametsr <span class="math inline">\(\varphi\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathcal{L}(\varphi) &amp;= \nabla_\varphi \mathbb{E}_{s \in \mathcal{S}} [ (V^\pi(s) - V_\varphi(s))^2] \\
    &amp;\\
    &amp; = \mathbb{E}_{s \in \mathcal{S}} [\nabla_\varphi  (V^\pi(s) - V_\varphi(s))^2] \\
    &amp;\\
    &amp; = \mathbb{E}_{s \in \mathcal{S}} [ - (V^\pi(s) - V_\varphi(s)) \, \nabla_\varphi V_\varphi(s)] \\
\end{aligned}
\]</span></p>
<p>As it would be too slow to compute the expectation on the whole state space (<strong>batch algorithm</strong>), we will update the parameters with <strong>stochastic gradient descent</strong> (SGD):</p>
<p><span class="math display">\[
    \Delta \varphi = \eta \,  \frac{1}{K} \sum_{k=1}^K (V^\pi(s_k) - V_\varphi(s_k)) \, \nabla_\varphi V_\varphi(s_k)
\]</span></p>
<p>where <span class="math inline">\(K\)</span> is the batch size. We can also sample a single state <span class="math inline">\(s\)</span> (online algorithm):</p>
<p><span class="math display">\[
    \Delta \varphi = \eta \, (V^\pi(s) - V_\varphi(s)) \, \nabla_\varphi V_\varphi(s)
\]</span></p>
<p>Unless stated otherwise, we will sample single states in this section, but the parameter updates will be noisy (high variance).</p>
<p>The obtained rule is the <strong>delta learning rule</strong> of linear regression and classification, with <span class="math inline">\(\phi(s)\)</span> being the input vector and <span class="math inline">\(V^\pi(s) - V_\varphi(s)\)</span> the prediction error. The rule can be used with any function approximator, we only need to be able to differentiate it to get <span class="math inline">\(\nabla_\varphi V_\varphi(s)\)</span>. The problem is that we do not know <span class="math inline">\(V^\pi(s)\)</span>, as it is what we are trying to estimate. We can replace <span class="math inline">\(V^\pi(s)\)</span> by a sampled estimate using Monte Carlo or TD:</p>
<ul>
<li><strong>Monte Carlo</strong> function approximation:</li>
</ul>
<p><span class="math display">\[
    \Delta \varphi = \eta \, (R_t - V_\varphi(s)) \, \nabla_\varphi V_\varphi(s)
\]</span></p>
<ul>
<li><strong>Temporal Difference</strong> function approximation:</li>
</ul>
<p><span class="math display">\[
    \Delta \varphi = \eta \, (r_{t+1} + \gamma \, V_\varphi(s') - V_\varphi(s)) \, \nabla_\varphi V_\varphi(s)
\]</span></p>
<p>Note that for Temporal Difference, we actually want to minimize the TD reward-prediction error for all states, i.e.&nbsp;the surprise:</p>
<p><span class="math display">\[\mathcal{L}(\varphi) = \mathbb{E}_{s \in \mathcal{S}} [ (r_{t+1} + \gamma \, V_\varphi(s') - V_\varphi(s))^2]= \mathbb{E}_{s \in \mathcal{S}} [ \delta_t^2]\]</span></p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Gradient Monte Carlo Algorithm for value estimation
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Algorithm:</p>
<ul>
<li><p>Initialize the parameter <span class="math inline">\(\varphi\)</span> to 0 or randomly.</p></li>
<li><p><strong>while</strong> not converged:</p>
<ol type="1">
<li>Generate an episode according to the current policy <span class="math inline">\(\pi\)</span> until a terminal state <span class="math inline">\(s_T\)</span> is reached.</li>
</ol>
<p><span class="math display">\[
      \tau = (s_o, a_o, r_ 1, s_1, a_1, \ldots, s_T)
  \]</span></p>
<ol start="2" type="1">
<li><p>For all encountered states <span class="math inline">\(s_0, s_1, \ldots, s_{T-1}\)</span>:</p>
<ol type="1">
<li><p>Compute the return <span class="math inline">\(R_t = \sum_k \gamma^k r_{t+k+1}\)</span> .</p></li>
<li><p>Update the parameters using function approximation:</p></li>
</ol>
<p><span class="math display">\[
     \Delta \varphi = \eta \, (R_t - V_\varphi(s_t)) \, \nabla_\varphi V_\varphi(s_t)
\]</span></p></li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
<p>Gradient Monte Carlo has no bias (real returns) but a high variance.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Semi-gradient Temporal Difference Algorithm for value estimation
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Algorithm:</p>
<ul>
<li><p>Initialize the parameter <span class="math inline">\(\varphi\)</span> to 0 or randomly.</p></li>
<li><p><strong>while</strong> not converged:</p>
<ul>
<li><p>Start from an initial state <span class="math inline">\(s_0\)</span>.</p></li>
<li><p><strong>foreach</strong> step <span class="math inline">\(t\)</span> of the episode:</p>
<ul>
<li><p>Select <span class="math inline">\(a_t\)</span> using the current policy <span class="math inline">\(\pi\)</span> in state <span class="math inline">\(s_t\)</span>.</p></li>
<li><p>Observe <span class="math inline">\(r_{t+1}\)</span> and <span class="math inline">\(s_{t+1}\)</span>.</p></li>
<li><p>Update the parameters using function approximation:</p></li>
</ul>
<p><span class="math display">\[
      \Delta \varphi = \eta \, (r_{t+1} + \gamma \, V_\varphi(s_{t+1}) - V_\varphi(s_t)) \, \nabla_\varphi V_\varphi(s_t)
  \]</span></p>
<ul>
<li><strong>if</strong> <span class="math inline">\(s_{t+1}\)</span> is terminal: <strong>break</strong></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<p>Semi-gradient TD has less variance, but a significant bias as <span class="math inline">\(V_\varphi(s_{t+1})\)</span> is initially wrong. You can never trust these estimates completely.</p>
</section>
<section id="action-value-approximators" class="level3">
<h3 class="anchored" data-anchor-id="action-value-approximators">Action value approximators</h3>
<p>Q-values can be approximated by a parameterized function <span class="math inline">\(Q_\theta(s, a)\)</span> in the same manner. There are basically two options for the structure of the function approximator:</p>
<ol type="1">
<li>The FA takes a feature vector for both the state <span class="math inline">\(s\)</span> and the action <span class="math inline">\(a\)</span> (which can be continuous) as inputs, and outputs a single Q-value <span class="math inline">\(Q_\theta(s ,a)\)</span>.</li>
</ol>
<div id="fig-qapprox1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qapprox1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/functionapproximation-action1.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-qapprox1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Single Q-value approximation.
</figcaption>
</figure>
</div>
<ol start="2" type="1">
<li>The FA takes a feature vector for the state <span class="math inline">\(s\)</span> as input, and outputs one Q-value <span class="math inline">\(Q_\theta(s ,a)\)</span> per possible action (the action space must be discrete).</li>
</ol>
<div id="fig-qapprox2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qapprox2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/functionapproximation-action2.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-qapprox2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Multiple Q-value approximation
</figcaption>
</figure>
</div>
<p>In both cases, we minimize the mse between the true value <span class="math inline">\(Q^\pi(s, a)\)</span> and the approximated value <span class="math inline">\(Q_\theta(s, a)\)</span>. The target can be approximated with SARSA or Q-learning:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q-learning with function approximation
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Initialize the parameters <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><strong>while</strong> True:</p>
<ul>
<li><p>Start from an initial state <span class="math inline">\(s_0\)</span>.</p></li>
<li><p><strong>foreach</strong> step <span class="math inline">\(t\)</span> of the episode:</p>
<ul>
<li><p>Select <span class="math inline">\(a_{t}\)</span> using the behavior policy <span class="math inline">\(b\)</span> (e.g.&nbsp;derived from <span class="math inline">\(\pi\)</span>).</p></li>
<li><p>Take <span class="math inline">\(a_t\)</span>, observe <span class="math inline">\(r_{t+1}\)</span> and <span class="math inline">\(s_{t+1}\)</span>.</p></li>
<li><p>Update the parameters <span class="math inline">\(\theta\)</span>:</p></li>
</ul>
<p><span class="math display">\[\Delta \theta = \eta \, (r_{t+1} + \gamma \, \max_a Q_\theta(s_{t+1}, a) - Q_\theta(s_t, a_t)) \, \nabla_\theta Q_\theta(s_t, a_t)\]</span></p>
<ul>
<li>Improve greedily the learned policy:</li>
</ul>
<p><span class="math display">\[\pi(s_t, a) = \text{Greedy}(Q_\theta(s_t, a))\]</span></p>
<ul>
<li><strong>if</strong> <span class="math inline">\(s_{t+1}\)</span> is terminal: <strong>break</strong></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</section>
</section>
<section id="policy-based-function-approximation" class="level2">
<h2 class="anchored" data-anchor-id="policy-based-function-approximation">Policy-based function approximation</h2>
<p>In policy-based function approximation, we want to directly learn a policy <span class="math inline">\(\pi_\theta(s, a)\)</span> that maximizes the expected return of each possible transition, i.e.&nbsp;the ones which are selected by the policy. The <strong>objective function</strong> to be maximized is defined over all trajectories <span class="math inline">\(\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, a_T)\)</span> conditioned by the policy:</p>
<p><span class="math display">\[
    \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \rho_\theta} [R (\tau)]
\]</span></p>
<p>In short, the learned policy <span class="math inline">\(\pi_\theta\)</span> should only produce trajectories <span class="math inline">\(\tau\)</span> where each state is associated to a high return <span class="math inline">\(R(\tau)\)</span> and avoid trajectories with low returns. Although this objective function leads to the desired behavior, it is not computationally tractable as we would need to integrate over all possible trajectories. The methods presented in Section <strong>?@sec-policygradient</strong> will provide estimates of the gradient of this objective function.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>