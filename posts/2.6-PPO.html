<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ppo</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="2.6-PPO_files/libs/clipboard/clipboard.min.js"></script>
<script src="2.6-PPO_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="2.6-PPO_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="2.6-PPO_files/libs/quarto-html/popper.min.js"></script>
<script src="2.6-PPO_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="2.6-PPO_files/libs/quarto-html/anchor.min.js"></script>
<link href="2.6-PPO_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="2.6-PPO_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="2.6-PPO_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="2.6-PPO_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="2.6-PPO_files/libs/bootstrap/bootstrap-bb8a42f168430693d1c0fc26666c4cdf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="policy-optimization-trpo-ppo" class="level1">
<h1>Policy optimization (TRPO, PPO)</h1>
<section id="trust-region-policy-optimization-trpo" class="level2">
<h2 class="anchored" data-anchor-id="trust-region-policy-optimization-trpo">Trust Region Policy Optimization (TRPO)</h2>
<section id="principle" class="level3">
<h3 class="anchored" data-anchor-id="principle">Principle</h3>
<p><span class="citation" data-cites="Schulman2015">@Schulman2015</span> extended the idea of natural gradients to allow their use for non-linear function approximators (e.g.&nbsp;deep networks), as the previous algorithms only worked efficiently for linear approximators. The proposed algorithm, Trust Region Policy Optimization (TRPO), has now been replaced in practice by Proximal Policy Optimization (PPO, see next section) but its novel ideas are important to understand already.</p>
<p>Let’s note the expected return of a policy <span class="math inline">\(\pi\)</span> as:</p>
<p><span class="math display">\[
    \eta(\pi) = \mathbb{E}_{s \sim \rho_\pi, a \sim \pi}[\sum_{t=0}^\infty \gamma^t \, r(s_t, a_t, s_{t+1})]
\]</span></p>
<p>where <span class="math inline">\(\rho_\pi\)</span> is the discounted visitation frequency distribution (the probability that a state <span class="math inline">\(s\)</span> will be visited at some point in time by the policy <span class="math inline">\(\pi\)</span>):</p>
<p><span class="math display">\[
    \rho_\pi(s) = P(s_0=s) + \gamma \, P(s_1=s) + \gamma^2 \, P(s_2=s) + \ldots
\]</span></p>
<p><span class="citation" data-cites="Kakade2002">@Kakade2002</span> had shown that it is possible to relate the expected return of two policies <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> using advantages (omitting <span class="math inline">\(\pi\)</span> in the notations):</p>
<p><span class="math display">\[
    \eta(\theta) = \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_\theta}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)]
\]</span></p>
<p>The advantage <span class="math inline">\(A_{\pi_{\theta_\text{old}}}(s, a)\)</span> denotes the change in the expected return obtained after <span class="math inline">\((s, a)\)</span> when using the new policy <span class="math inline">\(\pi_\theta\)</span>, in comparison to the one obtained with the old policy <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. While this formula seems interesting (it measures how good the new policy is with regard to the average performance of the old policy, so we could optimize directly), it is difficult to estimate as the mathematical expectation depends on state-action pairs generated by the new policy : <span class="math inline">\(s \sim \rho_{\pi_\theta}, a \sim \pi_\theta\)</span>.</p>
<p><span class="citation" data-cites="Schulman2015">@Schulman2015</span> propose an approximation to this formula, by considering that if the two policies <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> are not very different from another, one can sample the states from the old distribution:</p>
<p><span class="math display">\[
    \eta(\theta) \approx \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)]
\]</span></p>
<p>One can already recognize the main motivation behind natural gradients: finding a weight update that moves the policy in the right direction (getting more rewards) while keeping the change in the policy distribution as small as possible (to keep the assumption correct).</p>
<p>Let’s now define the following objective function:</p>
<p><span class="math display">\[
    J_{\theta_\text{old}}(\theta) = \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)]
\]</span></p>
<p>It is easy to see that <span class="math inline">\(J_{\theta_\text{old}}(\theta_\text{old}) = \eta(\theta_\text{old})\)</span> by definition of the advantage, and that its gradient w.r.t to <span class="math inline">\(\theta\)</span> taken in <span class="math inline">\(\theta_\text{old}\)</span> is the same as the one of <span class="math inline">\(\eta(\theta_\text{old})\)</span>:</p>
<p><span class="math display">\[
    \nabla_\theta J_{\theta_\text{old}}(\theta)|_{\theta = \theta_\text{old}} = \nabla_\theta \eta(\theta)|_{\theta = \theta_\text{old}}
\]</span></p>
<p>This means that, at least locally, one maximization step of <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> goes in the same direction as maximizing <span class="math inline">\(\eta(\theta)\)</span> if we do not go too far. <span class="math inline">\(J\)</span> is called a <strong>surrogate objective function</strong>: it is not what we want to optimize, but it leads to the same result. TRPO belongs to the class of <strong>minorization-majorization</strong> algorithms (MM, we first find a local lower bound and then maximize it, iteratively).</p>
<p>Let’s now suppose that we can find its maximum, i.e.&nbsp;a policy <span class="math inline">\(\pi'\)</span> that maximizes the advantage of each state-action pair over <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. There would be no guarantee that <span class="math inline">\(\pi'\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> are close enough so that the assumption stands. We could therefore make only a small step in its direction and hope for the best:</p>
<p><span id="eq-linesearch"><span class="math display">\[
    \pi_\theta(s, a) = (1-\alpha) \, \pi_{\theta_\text{old}}(s, a) + \alpha \, \pi'(s,a)
\tag{1}\]</span></span></p>
<p>This is the conservative policy iteration method of <span class="citation" data-cites="Kakade2002">@Kakade2002</span>, where a bound on the difference between <span class="math inline">\(\eta(\pi_{\theta_\text{old}})\)</span> and <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> can be derived.</p>
<p><span class="citation" data-cites="Schulman2015">@Schulman2015</span> propose to penalize instead the objective function by the KL divergence between the new and old policies. There are basically two ways to penalize an optimization problem:</p>
<ol type="1">
<li>Adding a hard constraint on the KL divergence, leading to a constrained optimization problem (where Lagrange methods can be applied):</li>
</ol>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad J_{\theta_\text{old}}(\theta) \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<ol start="2" type="1">
<li>Regularizing the objective function with the KL divergence:</li>
</ol>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad L(\theta) = J_{\theta_\text{old}}(\pi_\theta) - C \,  D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)
\]</span></p>
<p>In the first case, we force the KL divergence to stay below a certain threshold. In the second case, we penalize solutions that would maximize <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> but would be too different from the previous policy. In both cases, we want to find a policy <span class="math inline">\(\pi_\theta\)</span> maximizing the expected return (the objective), but which is still close (in terms of KL divergence) from the current one. Both methods are however sensible to the choice of the parameters <span class="math inline">\(\delta\)</span> and <span class="math inline">\(C\)</span>.</p>
<p>Formally, the KL divergence <span class="math inline">\(D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)\)</span> should be the maximum KL divergence over the state space:</p>
<p><span class="math display">\[
    D^\text{max}_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) = \max_s D_{KL}(\pi_{\theta_\text{old}}(s, .) || \pi_\theta(s, .))
\]</span></p>
<p>This maximum KL divergence over the state space would be very hard to compute. Empirical evaluations showed however that it is safe to use the mean KL divergence, or even to sample it:</p>
<p><span class="math display">\[
    \bar{D}_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) = \mathbb{E}_s [D_{KL}(\pi_{\theta_\text{old}}(s, .) || \pi_\theta(s, .))] \approx \frac{1}{N} \sum_{i=1}^N D_{KL}(\pi_{\theta_\text{old}}(s_i, .) || \pi_\theta(s_i, .))
\]</span></p>
</section>
<section id="trust-regions" class="level3">
<h3 class="anchored" data-anchor-id="trust-regions">Trust regions</h3>
<div id="fig-trustregion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trustregion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/trustregion.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trustregion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Graphical illustration of trust regions. From the current parameters <span class="math inline">\(\theta_\text{old}\)</span>, we search for the maximum <span class="math inline">\(\theta^*\)</span> of the real objective <span class="math inline">\(\eta(\theta)\)</span>. The unconstrained objective <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> is locally similar to <span class="math inline">\(\eta(\theta)\)</span> but quickly diverge as <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> become very different. The surrogate objective <span class="math inline">\(L(\theta) = J_{\theta_\text{old}} (\theta) - C \,  D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)\)</span> is always smaller than <span class="math inline">\(\eta(\theta)\)</span> and has a maximum close to <span class="math inline">\(\theta^*\)</span> which keeps <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> close from each other in terms of KL divergence. The region around <span class="math inline">\(\theta_\text{old}\)</span> where big optimization steps can be taken without changing the policy too much is called the trust region.
</figcaption>
</figure>
</div>
<p>Before diving further into how these optimization problems can be solved, let’s wonder why the algorithm is called <strong>trust region policy optimization</strong> using the regularized objective. <a href="#fig-trustregion" class="quarto-xref">Figure&nbsp;1</a> illustrates the idea. The “real” objective function <span class="math inline">\(\eta(\theta)\)</span> should be maximized (with gradient descent or similar) starting from the parameters <span class="math inline">\(\theta_\text{old}\)</span>. We cannot estimate the objective function directly, so we build a surrogate objective function <span class="math inline">\(L(\theta) = J_{\theta_\text{old}} (\theta) - C \,  D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)\)</span>. We know that:</p>
<ol type="1">
<li>The two objectives have the same value in <span class="math inline">\(\theta_\text{old}\)</span>: <span class="math display">\[
L(\theta_\text{old}) = J_{\theta_\text{old}}(\theta_\text{old}) - C \,  D_{KL}(\pi_{\theta_\text{old}} || \pi_{\theta_\text{old}}) = \eta(\theta_\text{old})
\]</span></li>
<li>Their gradient w.r.t <span class="math inline">\(\theta\)</span> are the same in <span class="math inline">\(\theta_\text{old}\)</span>: <span class="math display">\[
\nabla_\theta L(\theta)|_{\theta = \theta_\text{old}} = \nabla_\theta \eta(\theta)|_{\theta = \theta_\text{old}}
\]</span></li>
<li>The surrogate objective is always smaller than the real objective, as the KL divergence is positive: <span class="math display">\[
\eta(\theta) \geq J_{\theta_\text{old}}(\theta) - C \,  D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)
\]</span></li>
</ol>
<p>Under these conditions, the surrogate objective is also called a <strong>lower bound</strong> of the primary objective. The interesting fact is that the value of <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(L(\theta)\)</span> is at the same time:</p>
<ul>
<li>A big step in the parameter space towards the maximum of <span class="math inline">\(\eta(\theta)\)</span>, as <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta_\text{old}\)</span> can be very different.</li>
<li>A small step in the policy distribution space, as the KL divergence between the previous and the new policies is kept small.</li>
</ul>
<p>Exactly what we needed! The parameter region around <span class="math inline">\(\theta_\text{old}\)</span> where the KL divergence is kept small is called the <strong>trust region</strong>. This means that we can safely take big optimization steps (e.g.&nbsp;with a high learning rate or even analytically) without risking to violate the initial assumptions.</p>
</section>
<section id="sample-based-formulation" class="level3">
<h3 class="anchored" data-anchor-id="sample-based-formulation">Sample-based formulation</h3>
<p>Although the theoretical proofs in <span class="citation" data-cites="Schulman2015">@Schulman2015</span> used the regularized optimization method, the practical implementation uses the constrained optimization problem:</p>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad J_{\theta_\text{old}}(\theta) = \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)] \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<p>The first thing to notice is that <span class="math inline">\(\eta(\theta_\text{old})\)</span> does not depend on <span class="math inline">\(\theta\)</span>, so it is constant in the optimization problem. We only need to maximize the advantage of the actions taken by <span class="math inline">\(\pi_\theta\)</span> in each state visited by <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. The problem is that <span class="math inline">\(\pi_\theta\)</span> is what we search, so we can not sample actions from it. The solution is to use <strong>importance sampling</strong> to allow sampling actions from <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. This is possible as long as we correct the objective with the importance sampling weight:</p>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}} [\frac{\pi_\theta(s, a)}{\pi_{\theta_\text{old}}(s, a)} \, A_{\pi_{\theta_\text{old}}}(s, a)] \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<p>Now that states and actions are generated by the old policy, we can safely sample many trajectories using <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> (<span class="citation" data-cites="Schulman2015">@Schulman2015</span> proposes two methods called single path and Vine, but we ignore it here), compute the advantages of all state-action pairs (using real rewards along the trajectories), form the surrogate objective function and optimize it using second-order optimization methods.</p>
<p>One last thing to notice is that the advantages <span class="math inline">\(A_{\pi_{\theta_\text{old}}}(s, a) = Q_{\pi_{\theta_\text{old}}}(s, a) - V_{\pi_{\theta_\text{old}}}(s)\)</span> depend on the value of the states encountered by <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. The state values do not depend on the policies, they are constant for each optimization step, so they can also be safely removed:</p>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}} [\frac{\pi_\theta(s, a)}{\pi_{\theta_\text{old}}(s, a)} \, Q_{\pi_{\theta_\text{old}}}(s, a)] \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<p>Here we go, that’s TRPO. It could seem a bit disappointing to come up with such a simple formulation (find a policy which maximizes the Q-value of sampled actions while being not too different from the previous one) after so many mathematical steps, but that is also the beauty of it: not only it works, but it is guaranteed to work. With TRPO, each optimization step brings the policy closer from an optimum, what is called <strong>monotonic improvement</strong>.</p>
</section>
<section id="practical-implementation" class="level3">
<h3 class="anchored" data-anchor-id="practical-implementation">Practical implementation</h3>
<p>Now, how do we solve the constrained optimization problem? And what is the link with natural gradients?</p>
<p>To solve constrained optimization problems, we can form a Lagrange function with an additional parameter <span class="math inline">\(\lambda\)</span> and search for its maximum:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta, \lambda) = J_{\theta_\text{old}}(\theta)  - \lambda \, (D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) - \delta)
\]</span></p>
<p>Notice how close the Lagrange function is from the regularized problem used in the theory. We can form a first-order approximation of <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span>:</p>
<p><span class="math display">\[
    J_{\theta_\text{old}}(\theta) = \nabla_\theta J_{\theta_\text{old}}(\theta) \, (\theta- \theta_\text{old})
\]</span></p>
<p>as <span class="math inline">\(J_{\theta_\text{old}}(\theta_\text{old}) = 0\)</span>. <span class="math inline">\(g = \nabla_\theta J_{\theta_\text{old}}(\theta)\)</span> is the now familiar <strong>policy gradient</strong> with importance sampling. Higher-order terms do not matter, as they are going to be dominated by the KL divergence term.</p>
<p>We will use a second-order approximation of the KL divergence term using the Fisher Information Matrix:</p>
<p><span class="math display">\[
    D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) = (\theta- \theta_\text{old})^T \, F(\theta_\text{old}) \,  (\theta- \theta_\text{old})
\]</span></p>
<p>We get the following Lagrangian function:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta, \lambda) = \nabla_\theta J_{\theta_\text{old}}(\theta) \, (\theta- \theta_\text{old})  - \lambda \, (\theta- \theta_\text{old})^T \, F(\theta_\text{old}) \,  (\theta- \theta_\text{old})
\]</span></p>
<p>which is quadratic in <span class="math inline">\(\Delta \theta = \theta- \theta_\text{old}\)</span>. It has therefore a unique maximum, characterized by a first-order derivative equal to 0:</p>
<p><span class="math display">\[
    \nabla_\theta J_{\theta_\text{old}}(\theta) = \lambda \, F(\theta_\text{old}) \,  \Delta \theta
\]</span></p>
<p>or:</p>
<p><span class="math display">\[
    \Delta \theta  = \frac{1}{\lambda} \, F(\theta_\text{old})^{-1} \,  \nabla_\theta J_{\theta_\text{old}}(\theta)
\]</span></p>
<p>which is the <strong>natural gradient descent</strong>! The size of the step <span class="math inline">\(\frac{1}{\lambda}\)</span> still has to be determined, but it can also be replaced by a fixed hyperparameter.</p>
<p>The main problem is now to compute and inverse the Fisher information matrix, which is quadratic with the number of parameters <span class="math inline">\(\theta\)</span>, i.e.&nbsp;with the number of weights in the NN. <span class="citation" data-cites="Schulman2015">@Schulman2015</span> proposes to used <strong>conjugate gradients</strong> to iteratively approximate the Fisher, a second-order method which will not be presented here (see <a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf" class="uri">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a> for a detailed introduction). After the conjugate gradient optimization step, the constraint <span class="math inline">\(D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta\)</span> is however not ensured anymore, so a line search is made as in <a href="#eq-linesearch" class="quarto-xref">Equation&nbsp;1</a> until that criteria is met.</p>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p>TRPO is a policy gradient method using natural gradients to monotonically improve the expected return associated to the policy. As a minorization-maximization (MM) method, it uses a surrogate objective function (a lower bound on the expected return) to iteratively change the parameters of the policy using large steps, but without changing the policy too much (as measured by the KL divergence). Its main advantage over DDPG is that it is much less sensible to the choice of the learning rate.</p>
<p>However, it has several limitations:</p>
<ul>
<li>It is hard to use with neural networks having multiple outputs (e.g.&nbsp;the policy and the value function, as in actor-critic methods) as natural gradients are dependent on the policy distribution and its relationship with the parameters.</li>
<li>It works well when the NN has only fully-connected layers, but empirically performs poorly on tasks requiring convolutional layers or recurrent layers.</li>
<li>The use of conjugate gradients makes the implementation much more complicated and less flexible than regular SGD.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Additional resources
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="http://178.79.149.207/posts/trpo.html" class="uri">http://178.79.149.207/posts/trpo.html</a></li>
<li><a href="https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9" class="uri">https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9</a></li>
<li><a href="https://medium.com/@sanketgujar95/trust-region-policy-optimization-trpo-and-proximal-policy-optimization-ppo-e6e7075f39ed" class="uri">https://medium.com/@sanketgujar95/trust-region-policy-optimization-trpo-and-proximal-policy-optimization-ppo-e6e7075f39ed</a></li>
<li><a href="https://www.depthfirstlearning.com/2018/TRPO" class="uri">https://www.depthfirstlearning.com/2018/TRPO</a></li>
<li><a href="http://rll.berkeley.edu/deeprlcoursesp17/docs/lec5.pdf" class="uri">http://rll.berkeley.edu/deeprlcoursesp17/docs/lec5.pdf</a></li>
</ul>
</div>
</div>
</section>
</section>
<section id="sec-PPO" class="level2">
<h2 class="anchored" data-anchor-id="sec-PPO">Proximal Policy Optimization (PPO)</h2>
<p>Proximal Policy Optimization (PPO) was proposed by <span class="citation" data-cites="Schulman2017">@Schulman2017</span> to overcome the problems of TRPO (complexity, inability to share parameters or to use complex NN architectures) while increasing the range of tasks learnable by the system (compared to DQN) and improving the sample complexity (compared to online PG methods, which perform only one update per step).</p>
<p>For that, they investigated various surrogate objectives (lower bounds) that could be solved using first-order optimization techniques (gradient descent). Let’s rewrite the surrogate loss of TRPO in the following manner:</p>
<p><span class="math display">\[
    L^\text{CPI}(\theta) = \mathbb{E}_{t} [\frac{\pi_\theta(s_t, a_t)}{\pi_{\theta_\text{old}}(s_t, a_t)} \, A_{\pi_{\theta_\text{old}}}(s_t, a_t)] = \mathbb{E}_{t} [\rho_t(\theta) \, A_{\pi_{\theta_\text{old}}}(s_t, a_t)]
\]</span></p>
<p>by making the dependency over time explicit and noting the importance sampling weight <span class="math inline">\(\rho_t(\theta)\)</span>. The superscript CPI refers to conservative policy iteration <span class="citation" data-cites="Kakade2002">[@Kakade2002]</span>. Without a constraint, the maximization of <span class="math inline">\(L^\text{CPI}\)</span> would lead to an excessively large policy updates. The authors searched how to modify the objective, in order to penalize changes to the policy that make <span class="math inline">\(\rho_t(\theta)\)</span> very different from 1, i.e.&nbsp;where the KL divergence between the new and old policies would become high. They ended up with the following surrogate loss:</p>
<p><span class="math display">\[
    L^\text{CLIP}(\theta) = \mathbb{E}_{t} [ \min (\rho_t(\theta) \, A_{\pi_{\theta_\text{old}}}(s_t, a_t), \text{clip}(\rho_t(\theta) , 1- \epsilon, 1+\epsilon) \,  A_{\pi_{\theta_\text{old}}}(s_t, a_t)]
\]</span></p>
<p>The left part of the min operator is the surrogate objective of TRPO <span class="math inline">\(L^\text{CPI}(\theta)\)</span>. The right part restricts the importance sampling weight between <span class="math inline">\(1-\epsilon\)</span> and <span class="math inline">\(1 +\epsilon\)</span>. Let’s consider two cases (depicted on <a href="#fig-ppo" class="quarto-xref">Figure&nbsp;2</a>):</p>
<div id="fig-ppo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ppo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/ppo.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ppo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Illustration of the effect of clipping the importance sampling weight. Source: <span class="citation" data-cites="Schulman2017">@Schulman2017</span>.
</figcaption>
</figure>
</div>
<ol type="1">
<li><p>the transition <span class="math inline">\(s_t, a_t\)</span> has a positive advantage, i.e.&nbsp;it is a better action than expected. The probability of selecting that action again should be increased, i.e.&nbsp;<span class="math inline">\(\pi_\theta(s_t, a_t) &gt; \pi_{\theta_\text{old}}(s_t, a_t)\)</span>. However, the importance sampling weight could become very high (a change from 0.01 to 0.05 is a ration of <span class="math inline">\(\rho_t(\theta) = 5\)</span>). In that case, <span class="math inline">\(\rho_t(\theta)\)</span> will be clipped to <span class="math inline">\(1+\epsilon\)</span>, for example 1.2. As a consequence, the parameters <span class="math inline">\(\theta\)</span> will move in the right direction, but the distance between the new and the old policies will stay small.</p></li>
<li><p>the transition <span class="math inline">\(s_t, a_t\)</span> has a negative advantage, i.e.&nbsp;it is a worse action than expected. Its probability will be decreased and the importance sampling weight might become much smaller than 1. Clipping it to <span class="math inline">\(1-\epsilon\)</span> avoids drastic changes to the policy, while still going in the right direction.</p></li>
</ol>
<p>Finally, they take the minimum of the clipped and unclipped objective, so that the final objective is a lower bound of the unclipped objective. In the original paper, they use <strong>generalized advantage estimation</strong> (GAE, section <strong>?@sec-GAE</strong>) to estimate <span class="math inline">\(A_{\pi_{\theta_\text{old}}}(s_t, a_t)\)</span>, but anything could be used (n-steps, etc). Transitions are sampled by multiple actors in parallel, as in A2C.</p>
<p>The pseudo-algorithm of PPO is as follows:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
PPO algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Initialize an actor <span class="math inline">\(\pi_\theta\)</span> and a critic <span class="math inline">\(V_\varphi\)</span> with random weights.</p></li>
<li><p>while not converged :</p>
<ul>
<li><p>for <span class="math inline">\(N\)</span> actors in parallel:</p>
<ul>
<li><p>Collect <span class="math inline">\(T\)</span> transitions using <span class="math inline">\(\pi_\text{old}\)</span>.</p></li>
<li><p>Compute the generalized advantage of each transition using the critic.</p></li>
</ul></li>
<li><p>for <span class="math inline">\(K\)</span> epochs:</p>
<ul>
<li><p>Sample <span class="math inline">\(M\)</span> transitions from the ones previously collected.</p></li>
<li><p>Train the actor to maximize the clipped surrogate objective.</p></li>
</ul>
<p><span class="math display">\[
      L^\text{CLIP}(\theta) = \mathbb{E}_{t} [ \min (\rho_t(\theta) \, A_{\pi_{\theta_\text{old}}}(s_t, a_t), \text{clip}(\rho_t(\theta) , 1- \epsilon, 1+\epsilon) \,  A_{\pi_{\theta_\text{old}}}(s_t, a_t)]
  \]</span></p>
<ul>
<li>Train the critic to minimize the mse using TD learning.</li>
</ul></li>
<li><p><span class="math inline">\(\theta_\text{old} \leftarrow \theta\)</span></p></li>
</ul></li>
</ul>
</div>
</div>
<p>The main advantage of PPO with respect to TRPO is its simplicity: the clipped objective can be directly maximized using first-order methods like stochastic gradient descent or Adam. It does not depend on assumptions about the parameter space: CNNs and RNNs can be used for the policy. It is sample-efficient, as several epochs of parameter updates are performed between two transition samplings: the policy network therefore needs less fresh samples that strictly on-policy algorithms to converge.</p>
<p>The only drawbacks of PPO is that there no convergence guarantee (although in practice it converges more often than other state-of-the-art methods) and that the right value for <span class="math inline">\(\epsilon\)</span> has to be determined. PPO has improved the state-of-the-art on Atari games and Mujoco robotic tasks. It has become the go-to method for continuous control problems.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Additional resources
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>More explanations and demos from OpenAI: <a href="https://blog.openai.com/openai-baselines-ppo" class="uri">https://blog.openai.com/openai-baselines-ppo</a></li>
</ul>
</div>
</div>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/jwSbzNHGflM" title="PPO" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>